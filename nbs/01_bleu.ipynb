{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bleu\n",
    "> BLEU automated machine translation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "homedir = \"\"\n",
    "if running_in_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    homedir = \"/content/drive/MyDrive\"\n",
    "else:\n",
    "    homedir = os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if running_in_colab:\n",
    "    GITHUB_TEST_FOLDER = homedir+\"/github/polyglottech/mteval\"\n",
    "    %cd {GITHUB_TEST_FOLDER}\n",
    "    !pip3 install -e '.[dev]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sacrebleu.metrics import BLEU\n",
    "def measure_bleu(hypothesis_lines,reference_lines,targetlang,tok=None):\n",
    "    \"\"\"Measuring standard BLEU on set of hypothesis and references\"\"\"\n",
    "    bleu = BLEU(trg_lang=targetlang,tokenize=tok)\n",
    "    score = bleu.corpus_score(hypothesis_lines,[reference_lines])\n",
    "    sig = bleu.get_signature().format(short=False)\n",
    "    score_json = score.format(width=2,score_only=False,signature=sig,is_json=True)\n",
    "    return score_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "def measure_record_bleu(hypothesis_lines,reference_lines,sourcelang,targetlang,test_set_name,test_date,mtengine,score_pathname,score_fname,domain='',tok=None):\n",
    "    \"\"\"Score hypothesis with BLEU score and record it to a specified metrics file\"\"\"\n",
    "    score_json = measure_bleu(hypothesis_lines,reference_lines,targetlang,tok)\n",
    "\n",
    "    score_dict = json.loads(score_json)\n",
    "    metric_record = score_dict\n",
    "    metric_record[\"date\"] = test_date\n",
    "    metric_record[\"source_langid\"] = sourcelang\n",
    "    metric_record[\"target_langid\"] = targetlang\n",
    "    metric_record[\"test_set\"] = test_set_name\n",
    "    metric_record[\"engine\"] = mtengine\n",
    "    metric_record[\"domain\"] = domain\n",
    "    score_path = Path(score_pathname,score_fname)\n",
    "    with open(score_path,\"a\",newline=\"\") as score_file:\n",
    "        field_names = metric_record.keys()\n",
    "        writer = csv.DictWriter(score_file, fieldnames=field_names)\n",
    "        if score_file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metric_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mteval.dataset import *\n",
    "import json\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mteval_test_path = homedir+\"/mteval_test/\"\n",
    "source_seg, ref_seg = download_read_set(mteval_test_path,\"en\",\"it\",\"wmt09\")\n",
    "hyp_seg = ref_seg\n",
    "score = json.loads(measure_bleu(hyp_seg,ref_seg,\"it\"))\n",
    "# Only check a number of score results to verify a validly formatted result is returned\n",
    "assert score['name'] == 'BLEU'\n",
    "assert score['score'] == 100.0\n",
    "assert score['nrefs'] == '1'\n",
    "\n",
    "isodate = date.today().isoformat()\n",
    "score_filename = \"metrics_test.csv\"\n",
    "measure_record_bleu(hyp_seg,ref_seg,\"en\",\"it\",\"wmt09\",isodate,\"dummy\",mteval_test_path,score_filename)\n",
    "assert os.path.exists(mteval_test_path+score_filename) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
