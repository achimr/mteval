{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ter\n",
    "> TER Translation Error Rate automated machine translation evaluation\n",
    "\n",
    "> Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul, \"A Study of Translation Edit Rate with Targeted Human Annotation,\" Proceedings of Association for Machine Translation in the Americas, 2006. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "homedir = \"\"\n",
    "if running_in_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    homedir = \"/content/drive/MyDrive\"\n",
    "else:\n",
    "    homedir = os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if running_in_colab:\n",
    "    GITHUB_TEST_FOLDER = homedir+\"/github/polyglottech/mteval\"\n",
    "    %cd {GITHUB_TEST_FOLDER}\n",
    "    !pip3 install -e '.[dev]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sacrebleu.metrics import TER\n",
    "def measure_ter(hypothesis_lines,reference_lines,normalized=False,no_punct=False,asian_support=False,case_sensitive=False):\n",
    "    \"\"\"Measuring TER on set of hypotheses and references\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hypothesis_lines : List \n",
    "        Array of hypotheses.\n",
    "    reference_lines : List\n",
    "        Array of references.\n",
    "    normalized : bool, default=False\n",
    "        If `True`, applies basic tokenization to sentences.\n",
    "    no_punct : bool, default=False\n",
    "        If `True`, removes punctuations from sentences.\n",
    "    asian_support : bool, default=False\n",
    "        If `True`, adds support for Asian character processing.\n",
    "    case_sensitive : bool, default=False\n",
    "        If `True`, does not lowercase sentences.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple\n",
    "        Array of arrays containing TER score, number of edits and reference length and sacreBLEU TER score in JSON format.\n",
    "    \"\"\"\n",
    "    # Calculate corpus level score\n",
    "    ter = TER(normalized=normalized,no_punct=no_punct,asian_support=asian_support,case_sensitive=case_sensitive)\n",
    "    corpus_score = ter.corpus_score(hypothesis_lines,[reference_lines])\n",
    "    sig = ter.get_signature().format(short=False)\n",
    "    score_json = corpus_score.format(width=2,score_only=False,signature=sig,is_json=True)\n",
    "    \n",
    "    # Calculate segment level scores\n",
    "    seg_scores = []\n",
    "    for hypothesis, reference in zip(hypothesis_lines,reference_lines):\n",
    "        seg_score = ter.sentence_score(hypothesis,[reference])\n",
    "        seg_scores.append([seg_score.score,seg_score.num_edits,seg_score.ref_length])\n",
    "    return seg_scores, score_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "def measure_record_ter(hypothesis_lines,reference_lines,sourcelang,targetlang,test_set_name,test_date,mtengine,score_pathname,score_fname,domain='',normalized=False,no_punct=False,asian_support=False,case_sensitive=False):\n",
    "    \"\"\"Score hypothesis with TER score and record it to a specified metrics file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hypothesis_lines : List \n",
    "        Array of hypotheses.\n",
    "    reference_lines : List\n",
    "        Array of references.\n",
    "    sourcelang : str\n",
    "        Source language identifier.\n",
    "    targetlang : str\n",
    "        Target language identifier.\n",
    "    test_set_name : str\n",
    "        Name of test set.\n",
    "    test_date : str\n",
    "        Date of test as ISO-8601 formatted string.\n",
    "    mtengine : str\n",
    "        Name of the tested MT engine.\n",
    "    score_pathname : str\n",
    "        Path for results CSV file.\n",
    "    score_fname : str\n",
    "        File name for results CSV file.\n",
    "    domain : str, default=''\n",
    "        Domain string if applicable.\n",
    "    normalized : bool, default=False\n",
    "        If `True`, applies basic tokenization to sentences.\n",
    "    no_punct : bool, default=False\n",
    "        If `True`, removes punctuations from sentences.\n",
    "    asian_support : bool, default=False\n",
    "        If `True`, adds support for Asian character processing.\n",
    "    case_sensitive : bool, default=False\n",
    "        If `True`, does not lowercase sentences.\n",
    "    \"\"\"\n",
    "    seg_scores, score_json = measure_ter(hypothesis_lines,reference_lines,normalized,no_punct,asian_support,case_sensitive)\n",
    "\n",
    "    score_dict = json.loads(score_json)\n",
    "    metric_record = score_dict\n",
    "    metric_record[\"date\"] = test_date\n",
    "    metric_record[\"source_langid\"] = sourcelang\n",
    "    metric_record[\"target_langid\"] = targetlang\n",
    "    metric_record[\"test_set\"] = test_set_name\n",
    "    metric_record[\"engine\"] = mtengine\n",
    "    metric_record[\"domain\"] = domain\n",
    "    score_path = Path(score_pathname,score_fname)\n",
    "    with open(score_path,\"a\",newline=\"\") as score_file:\n",
    "        field_names = metric_record.keys()\n",
    "        writer = csv.DictWriter(score_file, fieldnames=field_names)\n",
    "        if score_file.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(metric_record)\n",
    "    \n",
    "    # Write segment level TER scores to plain text file\n",
    "    output_path = score_pathname+\"/\"+sourcelang+\"_\"+targetlang+\"/\"+test_date+\"/\"+test_set_name\n",
    "    os.makedirs(output_path,exist_ok=True)\n",
    "    seg_score_fname = \"\"\n",
    "    if domain:\n",
    "        seg_score_fname = \"ter_\"+mtengine+\".\"+domain+\".\"+sourcelang+\"-\"+targetlang\n",
    "    else:\n",
    "        seg_score_fname = \"ter_\"+mtengine+\".\"+sourcelang+\"-\"+targetlang\n",
    "    seg_score_path = Path(output_path,seg_score_fname)\n",
    "    with open(seg_score_path,\"w\",newline=\"\") as seg_score_fh:\n",
    "        writer = csv.writer(seg_score_fh)\n",
    "        for seg_score in seg_scores:\n",
    "            writer.writerow(seg_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from mteval.dataset import *\n",
    "import json\n",
    "import os\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mteval_test_path = homedir+\"/mteval_test/\"\n",
    "source_seg, ref_seg = download_read_set(mteval_test_path,\"en\",\"it\",\"wmt09\")\n",
    "hyp_seg = ref_seg\n",
    "seg_scores, score_json = measure_ter(hyp_seg,ref_seg)\n",
    "score = json.loads(score_json)\n",
    "# Only check a number of score results to verify a validly formatted result is returned\n",
    "assert score['name'] == 'TER'\n",
    "assert score['score'] == 0.0\n",
    "assert len(seg_scores) == 3027\n",
    "\n",
    "isodate = date.today().isoformat()\n",
    "score_filename = \"ter_test.csv\"\n",
    "measure_record_ter(hyp_seg,ref_seg,\"en\",\"it\",\"wmt09\",isodate,\"dummy\",mteval_test_path,score_filename)\n",
    "assert os.path.exists(mteval_test_path+score_filename) == 1\n",
    "assert os.path.exists(mteval_test_path+\"en_it/\"+isodate+\"/wmt09/ter_dummy.en-it\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
