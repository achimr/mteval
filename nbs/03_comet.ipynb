{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comet\n",
    "> Library to score machine translations with the COMET metric\n",
    ">\n",
    "> This library needs a CUDA-compatible GPU to execute (no check yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "if running_in_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    homedir = \"/content/drive/MyDrive\"\n",
    "else:\n",
    "    homedir = os.getenv('HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if running_in_colab:\n",
    "    github_test_folder = homedir+\"/github/polyglottech/mteval\"\n",
    "    %cd {github_test_folder}\n",
    "    !pip3 install nbdev\n",
    "    !pip3 install -e '.[dev]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if running_in_colab:\n",
    "    # Colab doesn't have a mechanism to set environment variables other than python-dotenv\n",
    "    env_file = homedir+'/secrets/.env'\n",
    "    %load_ext dotenv\n",
    "    %dotenv {env_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import comet\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "\n",
    "class cometscoring:\n",
    "    \"\"\"Class to calculate COMET score (with references)\"\"\"\n",
    "    def __init__(self,model_name=\"wmt20-comet-da\",gpus=1):\n",
    "        \"\"\"Constructor that downloads and loads the COMET model\"\"\"\n",
    "        model_path = download_model(model_name)\n",
    "        self.model = load_from_checkpoint(model_path)\n",
    "        self.gpus = gpus\n",
    "\n",
    "    def measure_comet(self,source_lines,hypothesis_lines,reference_lines):\n",
    "        \"\"\"Function to calculate the comet score\"\"\"\n",
    "        # Construct array necessary to measure COMET\n",
    "        comet_data = []\n",
    "        for (src,mt,ref) in zip(source_lines,hypothesis_lines,reference_lines):\n",
    "            comet_dict = {\"src\":src,\"mt\":mt,\"ref\":ref}\n",
    "            comet_data.append(comet_dict)\n",
    "        # Run COMET\n",
    "        seg_scores, sys_score = self.model.predict(comet_data, batch_size=8, gpus=self.gpus)\n",
    "\n",
    "        return seg_scores, sys_score\n",
    "\n",
    "    def measure_record_comet(self,source_lines,hypothesis_lines,reference_lines,sourcelang,targetlang,test_set_name,test_date,mtengine,score_pathname,score_fname,domain=''):\n",
    "        \"\"\"Function to score hypothesis with COMET score and record score to a specified metrics file\"\"\"           \n",
    "        seg_comet_scores, sys_score = self.measure_comet(source_lines,hypothesis_lines,reference_lines)\n",
    "\n",
    "        # Write the corpus COMET score with some meta data to CSV file\n",
    "        metric_record = {}\n",
    "        metric_record[\"name\"] = \"COMET\"\n",
    "        metric_record[\"score\"] = sys_score\n",
    "        metric_record[\"version\"] = comet.__version__\n",
    "        metric_record[\"date\"] = test_date\n",
    "        metric_record[\"source_langid\"] = sourcelang\n",
    "        metric_record[\"target_langid\"] = targetlang            \n",
    "        metric_record[\"test_set\"] = test_set_name\n",
    "        metric_record[\"engine\"] = mtengine\n",
    "        metric_record[\"domain\"] = domain\n",
    "        score_path = Path(score_pathname,score_fname)\n",
    "        with open(score_path,\"a\",newline=\"\") as score_file:\n",
    "            field_names = metric_record.keys()\n",
    "            writer = csv.DictWriter(score_file, fieldnames=field_names)\n",
    "            if score_file.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(metric_record)   \n",
    "\n",
    "        # Write segment level COMET scores to plain text file\n",
    "        output_path = score_pathname+\"/\"+sourcelang+\"_\"+targetlang+\"/\"+test_date+\"/\"+test_set_name\n",
    "        os.makedirs(output_path,exist_ok=True)\n",
    "        seg_score_fname = \"\"\n",
    "        if domain:\n",
    "            seg_score_fname = \"comet_\"+mtengine+\".\"+domain+\".\"+sourcelang+\"-\"+targetlang\n",
    "        else:\n",
    "            seg_score_fname = \"comet_\"+mtengine+\".\"+sourcelang+\"-\"+targetlang\n",
    "        seg_score_path = Path(output_path,seg_score_fname)\n",
    "        with open(seg_score_path,\"w\") as seg_score_fh:\n",
    "            for seg_comet_score in seg_comet_scores:\n",
    "                print(seg_comet_score,file=seg_score_fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import os\n",
    "import torch\n",
    "from mteval.dataset import *\n",
    "import json\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Testing with GPU on Github Actions is tricky and therefore not implemented yet\n",
    "# Unit test instead on Google Colab\n",
    "\n",
    "if not os.getenv('GITHUB_ACTIONS') == \"true\" and torch.cuda.is_available():\n",
    "    mteval_test_path = homedir+\"/mteval_test/\"\n",
    "    source_seg, ref_seg = download_read_set(mteval_test_path,\"en\",\"it\",\"wmt09\")\n",
    "    hyp_seg = ref_seg\n",
    "    comet_scorer = cometscoring()\n",
    "    seg_comet_scores, sys_comet = comet_scorer.measure_comet(source_seg,hyp_seg,ref_seg)\n",
    "    # Only check a number of score results to verify a validly formatted result is returned\n",
    "    assert sys_comet == 1.183892640986378\n",
    "    assert seg_comet_scores[0] == 1.2579313516616821\n",
    "    assert seg_comet_scores[10] == 1.142746090888977\n",
    "    assert seg_comet_scores[100] == 1.3046232461929321\n",
    "    assert seg_comet_scores[1000] == 1.2136374711990356\n",
    "    assert len(seg_comet_scores) == 3027\n",
    "\n",
    "    isodate = date.today().isoformat()\n",
    "    score_filename = \"comet_test.csv\"\n",
    "    comet_scorer.measure_record_comet(source_seg,hyp_seg,ref_seg,\"en\",\"it\",\"wmt09\",isodate,\"dummy\",mteval_test_path,score_filename)\n",
    "    assert os.path.exists(mteval_test_path+score_filename) == 1\n",
    "    assert os.path.exists(mteval_test_path+\"en_it/\"+isodate+\"/wmt09/comet_dummy.en-it\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(cometscoring.measure_comet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(cometscoring.measure_record_comet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
